好的，同学们！欢迎回到线性代数的精彩世界。今天我们要深入探讨一些更高级的概念，它们能帮助我们更好地理解和分析线性变换的结构。回想一下，我们一直在努力找到一个“最好”的基，使得线性变换的矩阵表示最简单。理想情况是对角矩阵，但这不总是可能的。今天，我们将探索为什么有些线性变换不能对角化，以及我们能达到的“次优”矩阵形式——若当标准型 (Jordan Canonical Form)，以及一个强大的工具——最小多项式 (Minimal Polynomial)，来帮助我们理解这些结构。

### 线性变换的值域与核 (Image and Kernel)

我们从回顾两个非常基础但极其重要的概念开始：线性变换的值域和核。它们就像是线性变换的“指纹”，揭示了变换的重要行为。

*   **直观理解:**
    *   **值域 (Image):** 想象一个线性变换 A 把向量从空间 V 映射到空间 W。值域就是所有**可能的输出向量**的集合。就像函数的值域一样，它是函数实际“覆盖”到的范围。记作 $A(V)$ 或 $\text{Im } A$。
    *   **核 (Kernel):** 想象线性变换 A 作用在 V 中的向量上。核就是所有被 A **变成零向量**的输入向量的集合。这些向量在变换下“消失”了。记作 $A^{-1}(0)$ 或 $\text{ker } A$。

*   **形式化定义 (定义 7.6.1):**
    *   设 V 是数域 P 上的线性空间，A 是 V 上的线性变换。
    *   **值域 $A(V) = \{A\xi \mid \xi \in V\}$**
    *   **核 $A^{-1}(0) = \{\alpha \mid A\alpha = 0\}$**

*   **重要性质:** 值域 $A(V)$ 和核 $A^{-1}(0)$ 都是线性空间 V 的**子空间 (subspace)**。
    *   **为什么是子空间？ (证明留作练习，但思路很重要):** 要证明一个集合是子空间，只需要证明它包含零向量，并且对加法和数乘封闭。
        *   **值域:** A(0) = 0，所以值域包含零向量。如果 $y_1, y_2$ 在值域里，意味着存在 $\xi_1, \xi_2 \in V$ 使得 $A\xi_1 = y_1, A\xi_2 = y_2$。那么 $y_1 + y_2 = A\xi_1 + A\xi_2 = A(\xi_1 + \xi_2)$。因为 $\xi_1 + \xi_2 \in V$，所以 $y_1+y_2$ 也在值域里。对于数乘 $ky_1 = k(A\xi_1) = A(k\xi_1)$。因为 $k\xi_1 \in V$，所以 $ky_1$ 也在值域里。
        *   **核:** A(0) = 0，所以零向量在核里。如果 $\alpha_1, \alpha_2$ 在核里，意味着 $A\alpha_1 = 0, A\alpha_2 = 0$。那么 $A(\alpha_1 + \alpha_2) = A\alpha_1 + A\alpha_2 = 0 + 0 = 0$，所以 $\alpha_1 + \alpha_2$ 也在核里。对于数乘 $kA\alpha_1 = A(k\alpha_1) = k0 = 0$，所以 $k\alpha_1$ 也在核里。

*   **秩 (Rank) 与零度 (Nullity):**
    *   $\text{dim } A(V)$ 称为线性变换 A 的**秩**。
    *   $\text{dim } A^{-1}(0)$ 称为线性变换 A 的**零度**。

*   **定理 7.6.1 (值域与核的求法):**
    *   选取 V 的一组基 $\epsilon_1, \dots, \epsilon_n$，A 在此基下的矩阵为 A。
    *   **(1) $A(V) = L(A\epsilon_1, A\epsilon_2, \dots, A\epsilon_n)$:** 值域就是基向量经过 A 变换后的向量组 $A\epsilon_1, \dots, A\epsilon_n$ 所张成的子空间。这很自然，因为 V 中任何向量 $\xi = \sum c_i \epsilon_i$ 经过 A 变换后是 $A\xi = A(\sum c_i \epsilon_i) = \sum c_i A\epsilon_i$，它是 $A\epsilon_i$ 的线性组合。
    *   **(3) 值域的基的坐标是矩阵 A 列的极大无关组:** 由于 $A\epsilon_i$ 向量的坐标是矩阵 A 的第 $i$ 列，所以 $A\epsilon_1, \dots, A\epsilon_n$ 张成的子空间就是矩阵 A 的列空间。值域的维数等于矩阵 A 的列秩，也就是矩阵 A 的秩。值域的一组基的坐标就是矩阵 A 列向量中的一个极大线性无关组。
    *   **(2) A 的秩等于 A 的秩:** 这是定理 (1) 和 (3) 的直接推论。
    *   **求核 (Ker A):** 核是满足 $A\alpha = 0$ 的向量 $\alpha$ 的集合。如果用坐标表示，设 $\alpha$ 在基下的坐标列向量是 $X$，那么 $A\alpha=0$ 对应矩阵方程 $AX=0$。核 $A^{-1}(0)$ 就是齐次线性方程组 $AX=0$ 的解空间。核的一组基就是 $AX=0$ 的一个基础解系。

*   **定理 7.6.2 (秩-零度定理):** 设 V 是 n 维线性空间，A 是 V 上的线性变换。
    **A 的秩 + A 的零度 = n**
    $\text{dim } A(V) + \text{dim } A^{-1}(0) = \text{dim } V$
    这个定理非常重要，它建立了值域和核的维数之间的基本关系。它的意义是：V 空间的维度 n，一部分“损失”在了核空间中（这些向量变成了零），剩余的维度就体现在了值域空间中。

*   **推论 (单射与满射):**
    *   **A 是单射 (一对一映射) $\iff A^{-1}(0) = \{0\}$:** 单射意味着不同的输入对应不同的输出。如果 $A\alpha_1 = A\alpha_2$，则 $\alpha_1 = \alpha_2$。这等价于 $A(\alpha_1 - \alpha_2) = 0 \implies \alpha_1 - \alpha_2 = 0$，即核空间只有零向量。
    *   **A 是满射 (映上映射) $\iff A(V) = V$:** 满射意味着值域覆盖了整个目标空间 V。当目标空间和定义域空间都是 V 时，A 是满射意味着值域的维数等于 V 的维数 n。
    *   **推论 3 (在有限维空间 V 到 V 的变换中):** A 是单射 $\iff$ A 是满射。这是因为根据秩-零度定理，单射 ($ \text{dim } A^{-1}(0) = 0$) 等价于秩等于 n ($ \text{dim } A(V) = n$)，而秩等于 n 正好是满射的条件 ($A(V)=V$)。**注意：这个推论只对定义域和目标空间同维度的线性变换成立！**

*   **值域与核的和:** 值域 $A(V)$ 和核 $A^{-1}(0)$ 都是 V 的子空间，它们的维数和等于 V 的维数 n。但是，$V = A(V) \oplus A^{-1}(0)$ (V 可以分解为值域和核的**直和**) 的充要条件是 $A(V) \cap A^{-1}(0) = \{0\}$。Note 中的例 1 可能说明存在 $A(V) \cap A^{-1}(0) \ne \{0\}$ 的情况，此时虽然维数和对了，但空间不能直和分解。

### 不变子空间 (Invariant Subspaces)

现在我们引入不变子空间的概念，它对于简化线性变换的矩阵表示至关重要。

*   **直观理解:** 一个子空间 W 是 A 的不变子空间，意味着如果一个向量在 W 中，对它应用线性变换 A 之后，结果向量仍然在 W 中。W 在 A 的作用下“封闭”或“不变”。

*   **形式化定义 (定义 7.7.1):** 设 V 是数域 P 上的线性空间，A 是 V 上的线性变换，W 是 V 的子空间。如果 $A(W) \subset W$，则称 W 为 A 的不变子空间 (A-子空间)。

*   **例子 (例 1-6):**
    *   平凡子空间 V 和 $\{0\}$ 总是 A 的不变子空间。
    *   **值域 $A(V)$ 和核 $A^{-1}(0)$ 都是 A 的不变子空间:**
        *   **核:** 如果 $\alpha \in A^{-1}(0)$，则 $A\alpha = 0$. 而 $0 \in A^{-1}(0)$ (因为 A(0)=0)。所以 $A(A^{-1}(0)) \subset A^{-1}(0)$。
        *   **值域:** 如果 $\eta \in A(V)$，则存在 $\xi \in V$ 使得 $A\xi = \eta$. 那么 $A\eta = A(A\xi) = A^2\xi$. 因为 $\xi \in V$, $A^2\xi$ 仍然是 A 作用在 V 某个向量 ($A\xi$) 上的结果，所以 $A^2\xi \in A(V)$. 因此 $A(A(V)) \subset A(V)$。
    *   如果 AB = BA (A 和 B 可交换)，则 B(V) 和 $B^{-1}(0)$ 是 A-子空间，A(V) 和 $A^{-1}(0)$ 是 B-子空间。 (思考为什么？对 B(V) 来说，如果 $y \in B(V)$，则 $y=B\xi$. $Ay = A(B\xi) = (AB)\xi = (BA)\xi = B(A\xi)$. 因为 $A\xi \in V$, $B(A\xi)$ 仍然是 B 作用在 V 某个向量上的结果，所以 $Ay \in B(V)$.)
    *   数乘变换 $A\xi = c\xi$ 的任意子空间都是不变子空间。（A 只是把所有向量拉伸/压缩，不改变方向（除了变零），所以子空间内的向量变换后还在该子空间方向上）。
    *   **一个子空间 W 是一维 A-子空间 $\iff$ W 由一个特征向量生成。** 如果 W 由非零向量 $\alpha$ 生成，W = Span($\alpha$)。 W 是 A-子空间意味着 $A\alpha \in W$, 即 $A\alpha = \lambda\alpha$ 对于某个标量 $\lambda$. 这正是 $\alpha$ 是 A 的特征向量的定义。对应的 $\lambda$ 是特征值。
    *   **属于特征值 $\lambda_0$ 的特征子空间 $V_{\lambda_0} = \{\alpha \mid A\alpha = \lambda_0\alpha\}$ 也是 A-子空间。** 如果 $\alpha \in V_{\lambda_0}$, $A\alpha = \lambda_0\alpha$. 再应用 A: $A(A\alpha) = A(\lambda_0\alpha) = \lambda_0 (A\alpha) = \lambda_0 (\lambda_0\alpha) = \lambda_0^2\alpha$. $A^2\alpha$ 仍然是 $\alpha$ 的倍数，在 Span($\alpha$) 内，更广泛地说，因为 $A\alpha$ 也属于 $V_{\lambda_0}$ (因为 $A(A\alpha) = \lambda_0(A\alpha)$), 所以 $A(V_{\lambda_0}) \subset V_{\lambda_0}$。
    *   微分变换 D 在多项式空间 $P[x]_n$ (次数不超过 n 的多项式) 上， $P[x]_n$ 是 D-子空间。 (对一个次数不超过 n 的多项式求导，结果仍然是次数不超过 n 的多项式)。

*   **诱导变换 (Induced Transformation) $A|_W$:** 如果 W 是 A 的不变子空间，我们可以把 A 限制在 W 上考虑。对于 W 中的向量 $\xi$, $A\xi$ 还在 W 中。所以 A 可以被看作 W 到 W 自身的一个线性变换，称为 A 在 W 上诱导出的变换 $A|_W$。
    *   例 7: $A|_{A^{-1}(0)}$ 是零变换 (因为它把核里的所有向量都变成零)。$A|_{V_{\lambda_0}}$ 是数乘变换 $\lambda_0 E$ (它把特征子空间里的所有向量都乘以 $\lambda_0$)。

*   **不变子空间与矩阵形式:** 不变子空间能简化线性变换的矩阵表示。
    *   **命题 2 (准上三角矩阵):** 如果 W 是 k 维 A-子空间，选取 W 的一组基 $\epsilon_1, \dots, \epsilon_k$，并将其扩充为 V 的一组基 $\epsilon_1, \dots, \epsilon_k, \epsilon_{k+1}, \dots, \epsilon_n$。那么 A 在这组基下的矩阵是**准上三角矩阵 (block upper triangular matrix)**：
        $$ \begin{pmatrix} A_1 & A_2 \\ 0 & A_3 \end{pmatrix} $$
        其中 $A_1$ 是 $k \times k$ 矩阵，表示 A 在 W 上的诱导变换 $A|_W$ 在基 $\epsilon_1, \dots, \epsilon_k$ 下的矩阵。矩阵左下角的 $0$ 块是 $(n-k) \times k$ 的零矩阵。
        *   **为什么是这样？** 矩阵的列表示基向量经过 A 变换后的坐标。对于前 k 个基向量 $\epsilon_1, \dots, \epsilon_k$，因为 W 是 A-子空间，$A\epsilon_i \in W$ 对于 $i=1, \dots, k$. $A\epsilon_i$ 是 $\epsilon_1, \dots, \epsilon_k$ 的线性组合，所以 $A\epsilon_i = \sum_{j=1}^k a_{ji} \epsilon_j$. 在矩阵表示中，这意味着第 $i$ 列的第 $k+1$ 行到第 n 行的元素都是零。这就是左下角为零的原因。$A\epsilon_i$ 在基 $\epsilon_1, \dots, \epsilon_n$ 下的坐标向量的前 k 个分量组成了 A1 的第 i 列。
    *   **(2) 空间分解为不变子空间的直和 (准对角矩阵):** 如果 V 可以分解为若干个 A-子空间的直和 $V = W_1 \oplus W_2 \oplus \dots \oplus W_s$，在每个 $W_i$ 中取一组基，将这些基合起来构成 V 的一组基。那么 A 在这组基下的矩阵是**准对角矩阵 (block diagonal matrix)**：
        $$ \begin{pmatrix} A_1 & & & \\ & A_2 & & \\ & & \ddots & \\ & & & A_s \end{pmatrix} $$
        其中 $A_i$ 是 A 在 $W_i$ 上的诱导变换 $A|_{W_i}$ 在 $W_i$ 的基下的矩阵。非对角线位置都是零块。
        *   **为什么？** 因为每个 $W_i$ 是 A-不变的，如果 $\xi_i \in W_i$, $A\xi_i \in W_i$. 如果 V 的基是由各 $W_i$ 的基简单拼接而成，那么 $A\xi_i$ 作为 V 中的向量，在由 V 的基表示时，只有属于 $W_i$ 的基向量的系数可能非零，属于其他 $W_j (j\ne i)$ 的基向量的系数都是零。这导致了矩阵的非对角块为零。

*   **结论 (定理 7.7.1):** 一个重要的结果是，在复数域 C 上，任何线性变换 A 的特征多项式 $f_A(x)$ 总是可以完全分解为一次因式的乘积：$f_A(x) = (x-\lambda_1)^{k_1} \dots (x-\lambda_s)^{k_s}$。此时，空间 V 可以分解为一些特殊的 A-子空间的直和：$V = V_1 \oplus V_2 \oplus \dots \oplus V_s$，其中 $V_i = \{\xi \mid (A-\lambda_i E)^{r_i}\xi = 0, \xi \in V\}$，这些 $V_i$ 称为**根子空间 (Root Subspace)** 或广义特征子空间 (Generalized Eigenspace)。$r_i$ 是某个正整数（具体值与若当标准型有关，但至少是 $k_i$）。这些根子空间都是 A-不变的。

*   **推论 (复数域上的线性变换):**
    *   复数域上的任何 n 维线性空间都可以分解为 A 的根子空间的直和。
    *   复数域上的任何线性变换（或任何复方阵）都相似于一个准对角矩阵，对角块对应于根子空间上的诱导变换。这也就是 Note 中提到的“准对角化”。

因此，能否找到一个使得线性变换矩阵是对角矩阵的基，归结于 V 能否分解为**特征子空间**的直和。如果分解为**根子空间**的直和，可以得到更一般的准对角矩阵。若当标准型是这种分解能达到的最精细的准对角形式。

### 若当标准型 (Jordan Canonical Form, JCF)

若当标准型是在线性变换无法对角化时，我们希望能达到的“最接近对角形”的矩阵形式。

*   **定义 (定义 7.8.1):**
    *   **若当块 (Jordan Block) $J(\lambda_0, k)$:** 是一个 $k \times k$ 的方阵，对角线上是同一个复数 $\lambda_0$，次对角线（正下方）全都是 1，其他地方都是 0。
        $$ J(\lambda_0, k) = \begin{pmatrix} \lambda_0 & 0 & \cdots & 0 \\ 1 & \lambda_0 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & 1 & \lambda_0 \end{pmatrix} $$
        Note 中的例子 1 展示了 $J(2,3)$ 和 $J(2+i, 3)$。
    *   **若当型矩阵 (Jordan Form Matrix):** 是由若干个若当块组成的准对角矩阵。
        $$ A = \begin{pmatrix} J(\lambda_1, k_1) & & & \\ & J(\lambda_2, k_2) & & \\ & & \ddots & \\ & & & J(\lambda_s, k_s) \end{pmatrix} $$
        Note 中的例子 2 展示了一个由多个若当块组成的若当标准型矩阵。

*   **核心结论 (结论 1, 2):**
    *   **结论 1:** 设 A 是复数域上 n 维线性空间 V 的线性变换，则在 V 中存在一组基，使得 A 在该基下的矩阵为**若当标准型**。而且，这个若当标准型除了若当块的次序外，由 A 唯一决定。
    *   **结论 2:** 复数域上任何 n 阶方阵都相似于一个若当标准型矩阵。对角线上的元素是矩阵 A 的全部特征值（按代数重数计算）。
    *   **意义:** 若当标准型是在复数域（或特征多项式可以完全分解的数域）上，线性变换能达到的最简矩阵形式。它包含了对角矩阵（若当块都是 $1 \times 1$ 的特例），并且**唯一**地刻画了线性变换的结构（除了基的选择）。它告诉你一个线性变换到底有多“偏离”对角化。次对角线上的 1 就是非对角化的“证据”。

*   **若当基 (Jordan Basis):** 能使矩阵变为若当标准型的这组基称为若当基。这组基不是任意取的，它们与线性变换 A 的性质紧密相关（特别是与 $(A-\lambda E)^k \xi = 0$ 这样的方程有关）。找到若当基通常比找到对角化基要复杂。

*   **若当块的性质 (例 3):** 单个若当块 $J(\lambda_0, n)$ 对应的线性变换 A 在其标准基 $\epsilon_1, \dots, \epsilon_n$ 下有特殊的性质：
    *   $A\epsilon_1 = \lambda_0\epsilon_1 + \epsilon_2$
    *   $A\epsilon_2 = \lambda_0\epsilon_2 + \epsilon_3$
    *   $\dots$
    *   $A\epsilon_{n-1} = \lambda_0\epsilon_{n-1} + \epsilon_n$
    *   $A\epsilon_n = \lambda_0\epsilon_n$
    (注意：Note 中矩阵形式是下三角 1，所以对应的变换关系是 $A\epsilon_1 = \lambda_0\epsilon_1$, $A\epsilon_2 = \epsilon_1 + \lambda_0\epsilon_2$, $A\epsilon_3 = \epsilon_2 + \lambda_0\epsilon_3$, ..., $A\epsilon_n = \epsilon_{n-1} + \lambda_0\epsilon_n$)
    *   **(1) 包含 $\epsilon_1$ 的 A-子空间只有 V 本身:** 从 $A\epsilon_1 = \lambda_0\epsilon_1 + \epsilon_2$ (或 $A\epsilon_1 = \lambda_0\epsilon_1$ if using upper diagonal 1s standard) 看，如果 W 包含 $\epsilon_1$ 且不变，应用 A 后 $A\epsilon_1$ 必须在 W 里，这就把 $\epsilon_2$ 拉进来了。继续应用 A，会把 $\epsilon_3, \dots, \epsilon_n$ 拉进来。最终整个 V 都被拉进 W。
    *   **(2) 任意非零 A-子空间都包含 $\epsilon_n$ (对应于 $A\epsilon_n = \lambda_0\epsilon_n$, $\epsilon_n$ 是特征向量):** 任何 A-子空间 W 在复数域上诱导的变换 $A|_W$ 必有特征值 $\lambda_0$ (这是唯一的特征值)，所以必有特征向量。这个特征向量也是原变换 A 的特征向量。A 的特征向量只能是 $\epsilon_n$ 的倍数（通过解 $AX=\lambda_0 X$ 得到）。所以 W 必须包含 $\epsilon_n$ 的倍数，如果 W 非零，则 $\epsilon_n \in W$.
    *   **(3) V 不能分解为两个非平凡 A-子空间的直和:** 因为任何非平凡子空间都包含 $\epsilon_n$，所以它们的交集不是 $\{0\}$，不能构成直和。
    这些性质说明，一个由单个若当块表示的线性变换，其空间结构非常“紧凑”，无法进一步分解为更小的非平凡不变子空间的直和。

### 最小多项式 (Minimal Polynomial)

最小多项式是描述线性变换行为的另一个重要工具，它与若当标准型紧密相连。

*   **定义 (定义 7.9.1):** 设 A 是数域 P 上的 n 阶方阵。数域 P 上以 A 为根（即 $f(A)=0$），次数最低，首一（最高次项系数为 1）的多项式 $f(x)$ 称为 A 的**最小多项式**，记为 $m_A(x)$。

*   **存在性与唯一性:** 最小多项式是存在且唯一的。存在性可以利用空间中多项式矩阵的性质或 Zorn 引理来证明（集合 $M=\{f(x) \mid f(A)=0\}$ 非空，在多项式除法意义下的偏序集存在极小元）。唯一性由带余除法保证：如果有两个最小多项式 $m_1(x), m_2(x)$，次数相同且首一，那么它们必须整除对方，所以只能相等。

*   **定理 7.9.1 (最小多项式的核心性质):** 设 $A$ 是 $n$ 阶方阵，$f(x) \in P[x]$。则 $f(x)$ 以 $A$ 为根（即 $f(A)=0$）的充要条件是 $m_A(x) \mid f(x)$ （$m_A(x)$ 整除 $f(x)$）。
    *   **证明思路:**
        *   **必要性 ($f(A)=0 \implies m_A(x) \mid f(x)$):** 用 $m_A(x)$ 去除 $f(x)$：$f(x) = m_A(x)q(x) + r(x)$，其中 $r(x)=0$ 或 $\partial r(x) < \partial m_A(x)$。代入 A：$f(A) = m_A(A)q(A) + r(A)$。因为 $f(A)=0$ 且 $m_A(A)=0$，所以 $r(A)=0$。如果 $r(x) \ne 0$，那么我们就找到了一个次数比 $m_A(x)$ 更低、以 A 为根的多项式，这与 $m_A(x)$ 是次数最低的定义矛盾。所以 $r(x)$ 必须等于 0，即 $m_A(x)$ 整除 $f(x)$。
        *   **充分性 ($m_A(x) \mid f(x) \implies f(A)=0$):** 如果 $f(x) = m_A(x)q(x)$，那么 $f(A) = m_A(A)q(A) = 0 \cdot q(A) = 0$。

*   **推论 1:** 根据 Cayley-Hamilton 定理，$f_A(x)$ 以 A 为根，所以 $m_A(x) \mid f_A(x)$。最小多项式是特征多项式的一个因式。

*   **最小多项式的求法:**
    *   **相似矩阵有相同的最小多项式:** 如果 $B = P^{-1}AP$, $f(A)=0 \iff f(PBP^{-1})=0 \iff P f(B) P^{-1}=0 \iff f(B)=0$. 所以以 A 为根的多项式集合与以 B 为根的多项式集合完全相同，它们的次数最低首一多项式自然也相同。这说明最小多项式也是线性变换本身的性质，不依赖于基的选择。
    *   **若当块的最小多项式 (例 3, 引理 3):** $k \times k$ 的若当块 $J(\lambda_0, k)$ 的特征多项式是 $f_J(x) = (x-\lambda_0)^k$。它的最小多项式是 $m_J(x) = (x-\lambda_0)^k$。也就是说，若当块的最小多项式等于其特征多项式。
        *   **证明思路:** $f_J(J)=0$ 显然成立。要证明 $m_J(x)=(x-\lambda_0)^k$，只需要证明任何次数小于 k 的 $(x-\lambda_0)^p$ 在代入 J 后不为零。计算 $(J-\lambda_0 E)^p$ 可以发现，对于 $p < k$, $(J-\lambda_0 E)^p$ 是一个次对角线有 1 的矩阵，不为零。只有当 $p=k$ 时，$(J-\lambda_0 E)^k$ 才是零矩阵。所以次数最低的以 J 为根的首一多项式是 $(x-\lambda_0)^k$.

*   **根的关系 (命题 2):** $m_A(x)$ 的根和 $f_A(x)$ 的根是完全相同的，它们都是 A 的特征值。
    *   **证明思路:** $m_A(x) \mid f_A(x)$ 保证了 $m_A$ 的根都是 $f_A$ 的根（特征值）。反过来，设 $\lambda_0$ 是 $f_A(x)$ 的根（特征值），则存在非零特征向量 $\alpha$ 使得 $A\alpha = \lambda_0\alpha$. 对任何多项式 $f(x) = c_k x^k + \dots + c_0$, $f(A)\alpha = (c_k A^k + \dots + c_0 E)\alpha = c_k A^k\alpha + \dots + c_0 E\alpha$. 由于 $A^j\alpha = \lambda_0^j\alpha$，所以 $f(A)\alpha = c_k \lambda_0^k \alpha + \dots + c_0 \alpha = (c_k \lambda_0^k + \dots + c_0)\alpha = f(\lambda_0)\alpha$.
    *   将 $f(x) = m_A(x)$ 代入，我们有 $m_A(A)\alpha = m_A(\lambda_0)\alpha$. 因为 $m_A(A)=0$ 且 $\alpha \ne 0$，所以 $m_A(\lambda_0)$ 必须等于 0。这意味着每个特征值 $\lambda_0$ 都是 $m_A(x)$ 的根。

*   **准对角矩阵的最小多项式 (引理 1, 2):** 如果矩阵 A 是由块对角矩阵组成 $A = \text{diag}(A_1, \dots, A_s)$，那么 A 的最小多项式是各个对角块 $A_i$ 的最小多项式的**最小公倍数 (LCM)**： $m_A(x) = \text{lcm}(m_{A_1}(x), \dots, m_{A_s}(x))$。
    *   **证明思路:** 设 $g(x) = \text{lcm}(m_{A_1}(x), \dots, m_{A_s}(x))$。那么每个 $m_{A_i}(x)$ 都整除 $g(x)$，所以 $g(A_i)=0$ 对所有 $i$ 成立。由于 $A$ 是块对角， $g(A) = \text{diag}(g(A_1), \dots, g(A_s)) = \text{diag}(0, \dots, 0) = 0$。所以 $g(x)$ 以 A 为根。再证明 $g(x)$ 是次数最低的。任何以 A 为根的多项式 $\varphi(x)$，必须满足 $\varphi(A)=0$，也就是 $\text{diag}(\varphi(A_1), \dots, \varphi(A_s)) = 0$。这意味着 $\varphi(A_i)=0$ 对所有 $i$ 成立。所以 $m_{A_i}(x)$ 都整除 $\varphi(x)$。根据最小公倍数的定义， $\text{lcm}(m_{A_1}(x), \dots, m_{A_s}(x))$ 必须整除 $\varphi(x)$。因此 $g(x)$ 整除任何以 A 为根的多项式，结合它是首一的，它就是 A 的最小多项式。

*   **利用特征多项式和最小多项式确定若当标准型:**
    *   **特征多项式 $f_A(x)$:** 决定了所有特征值及其**代数重数**（特征值 $\lambda$ 在 $f_A(x)$ 中的指数）。这个代数重数等于在若当标准型中特征值 $\lambda$ 出现的总次数。
    *   **最小多项式 $m_A(x)$:** 决定了每个特征值 $\lambda$ 对应的**最大若当块**的大小。如果 $m_A(x)$ 包含因子 $(x-\lambda)^k$，那么特征值 $\lambda$ 在若当标准型中对应的最大若当块的大小是 $k \times k$。
    *   **如何确定若当块结构？**
        *   首先，找到 $f_A(x)$ 并分解因式，得到所有特征值及其代数重数。
        *   然后，找到 $m_A(x)$。对于每个特征值 $\lambda$，找到 $(x-\lambda)$ 在 $m_A(x)$ 中的最高次幂 $k_\lambda$。这就是 $\lambda$ 对应最大若当块的大小。
        *   剩下的若当块大小需要通过代数重数和**几何重数**（特征子空间的维数，等于 $n - \text{rank}(A-\lambda E)$，也等于若当标准型中特征值 $\lambda$ 对应的若当块数量）来确定。假设特征值 $\lambda$ 的代数重数是 $K_\lambda$，几何重数是 $g_\lambda$。这意味着 $\lambda$ 对应总共有 $g_\lambda$ 个若当块。其中一个块大小是 $k_\lambda$，剩下的 $g_\lambda - 1$ 个块大小需要满足：所有这些块的大小之和等于 $K_\lambda$，且每个块大小都不超过 $k_\lambda$。通常还需要更高阶的核的维数 $d_p = \text{dim } \text{ker}(A-\lambda E)^p$ 来完全确定若当块的精确数量和大小分布（例如，大小至少为 p 的若当块数量等于 $d_p - d_{p-1}$）。
    *   **例 5:** 设 $f_A(x) = (x-1)^3 (x+1)^2 (x-2)^3 (x-3)^2 (x+3)^3$。总阶数 $n = 3+2+3+2+3 = 13$。
        *   特征值 $\lambda=1$，代数重数 3。
        *   特征值 $\lambda=-1$，代数重数 2。
        *   特征值 $\lambda=2$，代数重数 3。
        *   特征值 $\lambda=3$，代数重数 2。
        *   特征值 $\lambda=-3$，代数重数 3。
        *   **(1) $m_A(x) = (x-1)^2 (x+1)^2 (x-2)^1 (x-3)^1 (x+3)^2$**
            *   $\lambda=1$: $m_A$ 中 $(x-1)$ 次幂是 2。最大若当块 $2 \times 2$。代数重数 3。总共 3 个 1。一个 $2 \times 2$ 块，还剩 $3-2=1$ 个 1。需要一个 $1 \times 1$ 块。若当块是 $J(1,2), J(1,1)$。几何重数是 2 (块的数量)。
            *   $\lambda=-1$: $m_A$ 中 $(x+1)$ 次幂是 2。最大若当块 $2 \times 2$. 代数重数 2。总共 2 个 -1。一个 $2 \times 2$ 块。若当块是 $J(-1,2)$。几何重数是 1。
            *   $\lambda=2$: $m_A$ 中 $(x-2)$ 次幂是 1。最大若当块 $1 \times 1$. 代数重数 3。总共 3 个 2。三个 $1 \times 1$ 块。若当块是 $J(2,1), J(2,1), J(2,1)$。几何重数是 3。这对应 $\lambda=2$ 的特征空间维数是 3，此时矩阵在对应子空间可对角化。
            *   $\lambda=3$: $m_A$ 中 $(x-3)$ 次幂是 1。最大若当块 $1 \times 1$. 代数重数 2。总共 2 个 3。两个 $1 \times 1$ 块。若当块是 $J(3,1), J(3,1)$。几何重数是 2。
            *   $\lambda=-3$: $m_A$ 中 $(x+3)$ 次幂是 2。最大若当块 $2 \times 2$. 代数重数 3。总共 3 个 -3。一个 $2 \times 2$ 块，还剩 $3-2=1$ 个 -3。需要一个 $1 \times 1$ 块。若当块是 $J(-3,2), J(-3,1)$。几何重数是 2。
            若当标准型由这些块组成：$\text{diag}(J(1,2), J(1,1), J(-1,2), J(2,1), J(2,1), J(2,1), J(3,1), J(3,1), J(-3,2), J(-3,1))$。这与讲义中的示例矩阵结构一致。

        *   **(2) $m_A(x) = (x-1)^1 (x+1)^1 (x-2)^1 (x-3)^1 (x+3)^1$**
            所有因子次幂都是 1。意味着所有特征值对应的最大若当块都是 $1 \times 1$。所以所有若当块都是 $1 \times 1$ 的。若当标准型就是对角矩阵！
            对角线上是特征值，出现次数为其代数重数：diag(1, 1, 1, -1, -1, 2, 2, 2, 3, 3, -3, -3, -3)。几何重数等于代数重数。

*   **定理 7.9.3 (可对角化的充要条件):** 矩阵 A 可对角化的充分必要条件是 $m_A(x)$ 可以分解为互异的一次因式的乘积（即 $m_A(x)$ 没有重根）。
    *   **证明思路:** A 可对角化当且仅当若当标准型中所有若当块都是 $1 \times 1$ 的。这等价于对于每个特征值 $\lambda$，其对应的最大若当块大小是 $1 \times 1$。根据最小多项式的性质，这等价于 $(x-\lambda)$ 在 $m_A(x)$ 中的次幂是 1。这必须对所有特征值成立，所以 $m_A(x)$ 必须是 $(x-\lambda_1)(x-\lambda_2)\dots(x-\lambda_s)$ 的形式，其中 $\lambda_i$ 是互不相同的特征值。

*   **例 6 ($A^2=A$):** 如果 $A^2=A$，则多项式 $f(x) = x^2-x$ 以 A 为根。根据定理 7.9.1，$m_A(x)$ 必须整除 $x^2-x = x(x-1)$。所以 $m_A(x)$ 可能是 $x$, $x-1$, $x(x-1)$。
    *   如果 $m_A(x) = x$，则 $A=0$。这是一个对角矩阵。
    *   如果 $m_A(x) = x-1$，则 $A-E=0 \implies A=E$。这是一个对角矩阵。
    *   如果 $m_A(x) = x(x-1)$，它的根是 0 和 1，是互不相同的实数。根据定理 7.9.3，$m_A(x)$ 分解为互异一次因式乘积，所以 A 可以对角化。
    在所有情况下，A 都是可对角化的。对应的对角矩阵对角线元素只能是 0 或 1（因为 $A^2=A$ 意味着 A 的特征值 $\lambda$ 满足 $\lambda^2=\lambda$，所以 $\lambda=0$ 或 $\lambda=1$）。对角阵的秩等于对角线上非零元素的个数（即 1 的个数）。

*   **推论 (复矩阵可对角化):** 复矩阵 A 可对角化的充分必要条件是其最小多项式 $m_A(x)$ 没有重根。这与定理 7.9.3 是等价的，因为在复数域上，任何多项式都可以完全分解为一次因式的乘积。

### 总结与要点提炼

*   **值域 (Im A) 和核 (Ker A):** 线性变换的输出空间和映射到零的输入空间，都是子空间。**秩-零度定理**连接了它们的维数。
*   **不变子空间 (A-子空间 W):** A 作用在 W 中的向量上，结果仍在 W 中 ($A(W) \subset W$)。它们使得矩阵表示可以简化为**准上三角**或**准对角**形式。值域、核、特征子空间、根子空间都是重要的不变子空间。
*   **空间分解为直和 $\iff$ 矩阵准对角化:** 如果 V 能分解为不变子空间的直和 $V = W_1 \oplus \dots \oplus W_s$，则矩阵可以化为以 $A|_{W_i}$ 为对角块的准对角矩阵。在复数域上，V 总是可以分解为根子空间的直和。
*   **若当标准型 (JCF):** 在复数域上，线性变换能达到的最简单的**准对角**矩阵形式，对角块是若当块 $J(\lambda, k)$。它由特征值、对角线上的 1 和 0 构成，唯一刻画了线性变换的结构。
*   **最小多项式 $m_A(x)$:** 以 A 为根的次数最低的首一多项式。它是矩阵的一个重要**不变量**。
*   **$m_A(x)$ 与 $f_A(x)$ 的关系:** $m_A(x) \mid f_A(x)$；它们的根相同，都是 A 的特征值。
*   **$m_A(x)$ 与 JCF 的关系:** $m_A(x)$ 的因子 $(x-\lambda)$ 的最高次幂，等于特征值 $\lambda$ 在 JCF 中对应的**最大若当块**的大小。
*   **可对角化的充要条件:** $m_A(x)$ 能分解为互异的一次因式乘积。

这些概念层层递进，从基础的核与值域，到允许矩阵分块的不变子空间，再到最精细的标准型——若当标准型，最后引入最小多项式作为工具来理解和确定若当标准型的结构。它们共同构成了理解线性变换深层性质和简化矩阵表示的理论框架。

### 思考与练习

Note 中的练习题是很好的巩固机会。比如练习 2 和 3 让你根据特征多项式和最小多项式来推导若当标准型的结构，这是本节最重要的应用之一。练习 4 让你在一个具体的线性空间和线性变换（多项式上的微分）上求最小多项式。练习 5 则是一个深刻的定理：在复数域上，任何 n 维空间的线性变换都存在任意维度的不变子空间。

希望今天的讲解能帮助大家理清这些概念之间的联系，抓住它们的核心思想。线性代数的魅力就在于这些抽象结构背后蕴含的深刻几何和代数意义。请大家课后务必多加练习，动手计算，才能真正掌握！============